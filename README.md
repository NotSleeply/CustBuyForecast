# 赛事主页
## 一、赛事背景

随着人工智能技术在各行业的深度渗透，AI已成为推动产业变革的核心驱动力。阿里云天池平台作为国内领先的AI竞赛社区，计划推出"天池极客挑战-AI×行业"系列活动，旨在构建"课-赛-证"专题训练，促进AI技术与热门垂直行业领域的深度融合。此赛事为电商专场——用户购买行为预测。

## 二、赛事安排

### 1、报名要求

- **赛事官网**：https://tianchi.aliyun.com/specials/promotion/ai/geekchallenge
- **参赛对象**：个人赛，面向国内泛开发者人群免费开放报名，不限专业、年龄、学历背景；

### 2、赛程设置

- **竞赛设置**：在线个人打榜赛，本次赛事分为周赛、月赛，使用同一个排行榜进行滚动排名。

| 阶段  |        赛事时间        | 获奖公示时间  |
| :---: | :--------------------: | :-----------: |
| 周赛1 | 8月14日0点-8月20日24点 | 8月21日12点前 |
| 周赛2 | 8月21日0点-8月27日24点 | 8月28日12点前 |
| 周赛3 | 8月28日0点-9月3日24点  | 9月04日12点前 |
| 周赛4 | 9月04日0点-9月10日24点 | 9月11日12点前 |
| 月赛  | 8月14日0点-9月14日24点 | 赛后2个工作日 |

- **参赛要求**：选手报名后，可根据时间安排，自由参加周赛、月赛
- **提交评测及排行榜更新**：赛事期间每日有5次提交评测机会，评测成功后，可前往【我的成绩】查看，排行榜在每日12-24点整点刷新（即12、13、14、15、16、17、18、19、20、21、22、23、24点）

温馨提示：周赛4结束后仍可以继续打榜，直至本期赛事结束，评选月赛奖项。

# 赛题与数据说明页
## 一、赛题背景

随着全球化进程的加速，跨境电商已成为无数创业者追逐的热门赛道。无论是追求全球市场的广阔商机，还是希望通过创新商业模式实现个人价值，跨境电商都提供了无限可能。然而在跨国市场拓展过程中，企业普遍面临跨文化营销策略的本地化适配挑战。尽管部分企业已成功实现单一市场的商业闭环，但多市场协同运营的复杂性仍显著存在，这主要源于不同地域消费者行为模式、支付习惯及消费决策路径的显著差异。

作为中国跨境电商领域的标杆企业，阿里巴巴旗下全球速卖通（AliExpress）自2010年成立以来，历经十五年高速发展，已构建起成熟的跨境电子商务生态系统。该平台现覆盖全球 220 个国家和地区，运营着18个语言版本的本地化站点，商品品类矩阵覆盖22个核心消费品类，形成从日常快消品到耐用品的全品类供给体系。在核心市场布局方面，平台已形成以俄罗斯、美国、西班牙、巴西、法国为主的成熟市场集群。

**本次比赛基于某国家在若干日内的用户购买数据，数据集分为A、B两部分，A为训练数据，B为预测数据（去除其最后一条购买数据）。要求参赛选手预测B部分用户被移除的最后一条行为数据。**

## 二、赛题数据

**训练数据**：A部分用户的购买数据。数据的整体统计信息如下：

| 记录数  | 买家数 |
| :-----: | :----: |
| 6989817 | 483117 |

**测试数据**：给出B部分用户的购买数据除掉最后一条。数据的整体统计信息如下：

| 记录数 | 买家数 |
| :----: | :----: |
| 140380 | 10576  |

商品属性表：数据中共涉及1873173个商品，对于其中大部分商品，我们都会给出该商品的类目id、店铺id以及加密价格，其中价格的加密函数f(x)为一个单调增函数。

商品属性表，训练数据，测试数据对应的文件列表为：item_attr.csv, train.csv和test_without_last.csv。

#### 数据格式:

无论是训练数据还是测试数据，都具有如下的格式：

| buyer_country_id | buyer_admin_id | item_id |  create_order_time  | irank |
| :--------------: | :------------: | :-----: | :-----------------: | :---: |
|        xx        |     817731     | 4033525 | 2018-06-12 07:12:58 |   1   |
|        xx        |     817731     |  98120  | 2018-06-11 07:12:58 |   2   |

其中各字段含义如下：

buyer_country_id: 买家国家id, 只有'xx'一种取值

buyer_admin_id: 买家id

item_id: 商品id

create_order_time: 订单创建时间

irank: 每个买家对应的所有记录按照时间顺序的逆排序

数据集特点:

1. 每个用户有至少7条购买数据
2. 测试数据中每个用户的最后一条购买数据所对应的商品一定在训练数据中出现过.
3. 训练数据与测试数据中的用户集交为空

#### 要求选手提交的数据

关于B部分用户每个用户的最后一条购买数据的预测Top30

### 提交说明

请选手提交的文件命名为username.csv, 其格式如下：

buyer_admin_id,predict 1,predict 2,......,predict 30

其中buyer_admin_id为买家id, predict 1 ,..., predict 30为预测用户购买商品Top30的item_id依概率从高到低排序。例如:

1233434,4354,23432,6546,...,91343

2132133,154,20987,34349,...,78772

### 评估方法

**MRR(Mean Reciprocal Rank)**
首先对选手提交的表格中的每个用户计算用户得分
MRR是一个常用的评估指标，特别是在**推荐系统**和**信息检索**领域。它的核心思想是：**你预测得越靠前，得分就越高。**

#### 如何使用MRR进行评估？

假设我们有一组用户的预测结果和真实数据：

- **用户A**：真实目标是商品X。模型预测列表为 [商品B, 商品X, 商品C, ...]。商品X排在第2位。
  - 用户A的 `score` = `1/2` = `0.5`
- **用户B**：真实目标是商品Y。模型预测列表为 [商品Y, 商品A, 商品D, ...]。商品Y排在第1位。
  - 用户B的 `score` = `1/1` = `1.0`
- **用户C**：真实目标是商品Z。模型预测列表为 [商品A, 商品B, 商品C, ...]。商品Z没有在列表中。
  - 用户C的 `score` = `0`

最终的MRR就是所有用户得分的平均值：`MRR = (0.5 + 1.0 + 0) / 3 ≈ 0.5`。

## 三、BaseLine参考

**赛事由【安泰杯——跨境电商智能算法大赛】改编而来，baseline可供参考：**
前往查看：https://tianchi.aliyun.com/notebook/63042

# 计划步骤

## 入门核心：从 “数据探索” 开始（最关键的第一步）
### 评估指标是什么?
- **重要的评估指标**：MRR，即平均倒数排名（Mean Reciprocal Rank）

### 探索性数据分析（EDA），这是零基础建立对比赛认知的核心：

- 数据清洗（解决 “数据能用吗”）
    处理缺失值：如用户年龄字段缺失，判断是填充（均值 / 中位数）还是直接删除。
    处理异常值：如订单金额为负数，需确认是数据错误还是特殊情况，通常删除异常值。
    处理重复值：如同一用户同一时间的重复行为记录，保留一条即可。
- 数据探索（解决 “数据能告诉什么”）
    单变量分析：统计用户行为分布（如浏览、加购、下单的用户占比）、商品销量 Top10 品类等，用柱状图 / 饼图可视化。
    多变量分析：探索影响购买的因素，如 “用户浏览次数与下单率的关系”“商品价格区间与销量的关系”，用折线图 / 热力图分析。
    时间趋势分析：若数据含时间维度（如近 30 天行为），看购买行为是否有周期性（如周末下单量更高），为特征工程提供方向。  
- 探索数据（EDA）： 下载并打开数据文件，用Python的Pandas库做初步探索。你需要了解数据的基本情况：
  - 数据量多大？
  - 有多少用户和商品？
  - 哪些列是类别型数据（如行为类型、商品类别），哪些是数值型数据（如时间）？
  - 是否有缺失值？
  - 用户行为随时间有什么变化规律？
  - 正负样本的比例如何？（比如，购买行为相比浏览行为是不是非常少？这会影响你选择模型和评估指标。）

## 构建基线模型(快速出分)

### 当你对数据有了初步认识后，不要急着去做复杂模型，先快速搭建一个简单的基线（Baseline）。基线模型就像一个起点，它可以帮你熟悉整个比赛流程，并得到一个初步分数，给自己一些信心。

1. 常用模型：

  - 传统机器学习： 逻辑回归、支持向量机、决策树、随机森林等。

  - 梯度提升树（GBDT）系列： XGBoost、LightGBM、CatBoost是比赛中的“三大件”，它们效果好、速度快、可解释性强，是你的首选。

2. 特征工程：特征工程是模型效果的“上限”，比算法本身更重要。在用户行为预测这类问题中，常用的特征主要围绕“用户-商品-时间”

  - 用户特征： 统计用户的历史行为、总消费金额、活跃天数等。
  - 商品特征： 统计商品。
  - 时间特征： 从时间戳中提取信息，比如星期几、一天中的第几个小时、距离上次购买的天数、距离上次加购的天数等。
  - 交叉特征： 将用户和商品的特征组合起来，比如“该用户是否购买过这类商品”、“该用户是否在某个时间段购买过商品”等。


## 进阶操作：特征工程与基础建模
### 当数据探索清晰后，进入 “如何构建预测模型” 阶段，零基础优先从 “简单有效” 的方法入手：

- 特征工程（解决 “给模型喂什么信息”）
    核心是将原始数据转化为 “模型能理解的特征”，针对 “用户购买行为预测”，可优先构建 3 类特征：
      用户特征：用户近 7/14/30 天的浏览次数、加购次数、下单次数，用户历史平均购买间隔等。
      商品特征：商品近 7 天的被浏览次数、加购次数、转化率（下单数 / 浏览数），商品所属品类等。
      行为时序特征：用户对某商品的最近一次浏览时间距预测日的天数、用户近 3 天内是否有过加购行为等。
    Tips：用 pandas 的 groupby（分组统计）、shift（时间偏移）等函数实现，无需复杂算法。
    
- 基础建模（解决 “用什么模型预测”）
    零基础避免直接用深度学习，优先选择 “易实现、易调参” 的传统模型：
    
    模型选择：
    	若预测 “是否购买”（二分类问题）：用逻辑回归（Logistic Regression）、随机森林（Random Forest），scikit-learn 库可直接调用，代码量极少。
    	若预测 “购买数量”（回归问题）：用线性回归、梯度提升树（XGBoost，需额外安装 xgboost 库，但调参简单，效果优于基础模型）。
    建模流程：
    	划分数据集：用 train_test_split 函数将数据分为训练集（80%，用于训练模型）和测试集（20%，用于验证效果）。
    	模型训练：调用模型库，用训练集拟合模型（如model.fit(X_train, y_train)）。
    	模型评估：用测试集计算评价指标（如model.score(X_test, y_test)），若效果差，返回特征工程阶段优化（如增加新特征、调整特征统计周期）。

## 优化模型
1. 更复杂的特征工程：

  - RFM模型： Recency（最近一次购买时间）、Frequency（购买频率）、Monetary（购买金额）。
  - 行为序列特征： 考虑用户行为的顺序性，比如用行为序列来构建特征，或使用更高级的序列模型。
  - Embedding（嵌入）： 将离散的ID（如用户ID、商品ID）转换为低维稠密的向量表示，这些向量可以作为深度学习模型的输入。

2. 模型融合（Model Ensemble）：

  - 将多个不同模型的预测结果进行加权平均或堆叠（Stacking），通常能获得比单个模型更好的效果。
  - 你可以将XGBoost、LightGBM、逻辑回归等模型的预测结果作为新的特征，再用一个模型（如逻辑回归）进行二次训练，这就是一种简单的Stacking。


## 关键辅助：善用赛事资源（少走 90% 的弯路）
零基础参赛的核心是 “不闭门造车”，充分利用天池平台的资源：

- 逛【论坛】板块
  赛事论坛通常会有官方发布的 “新手引导帖”“数据解读帖”，以及往届选手分享的 “入门思路”“代码开源”（如 GitHub 链接），直接参考他人的特征工程和建模逻辑，避免从零开始踩坑。
- 参考 “学习赛” 案例
  该赛事标注为 “学习赛”，天池平台通常会为学习赛提供 “ baseline 代码”（即基础可运行的参赛代码，包含数据处理、建模全流程），拿到后先逐行理解代码逻辑，再基于此修改（如增加自己想到的特征、更换模型），这是最快的入门方式。
- 主动提问
  若遇到具体问题（如 “某字段缺失值怎么处理”“模型准确率上不去怎么办”），可在论坛发帖提问，注明自己的操作步骤和遇到的问题，通常会有热心选手或官方答疑。

## 时间规划（按 “未开始” 状态提前准备）
- 赛前 1-2 周：学习 Python 基础 + 数据分析工具（pandas、matplotlib），熟悉天池平台操作。
- 赛事开始后 1 周内：完成赛题阅读、数据下载与初步清洗，输出 1 份 EDA 报告（含关键图表和结论）。
- 赛事中期 2-3 周：完成特征工程构建，用基础模型（如随机森林）跑出第一版预测结果，对比评价指标优化方向。
- 赛事后期 1 周：尝试调参（如调整随机森林的树数量）或更换模型（如 XGBoost），优化预测效果，按官方要求提交结果。

零基础无需追求 “拿名次”，重点在于通过完整流程掌握 “数据分析 - 建模” 的实战逻辑，为秋招积累项目经验，这才是学习赛的核心价值。

## 模型选择

- 这是一个非常典型的**序列推荐（Sequential Recommendation）**或**下一个项目预测（Next-Item Prediction）**任务。

- 任务目标是：根据用户过去的购买序列，预测其**最后一次**购买的商品。

### 1. 基于深度学习的序列模型

这类模型特别擅长捕捉用户行为序列中的时序模式和依赖关系。

- **RNN (循环神经网络) 或 GRU (门控循环单元)**：
  - **原理**：将用户的历史购买序列（`item_id` 序列）作为输入，通过循环结构来学习序列中的模式。
  - **优势**：能够捕捉到短期和长期的序列依赖，例如“买了手机壳后通常会买钢化膜”。
  - **如何应用**：每个用户是一个独立的序列，模型的输出层预测下一个最可能购买的商品。在您的任务中，就是预测序列的最后一个商品。
  - **代表模型**：GRU4Rec（基于GRU的推荐模型）。
- **Transformer (自注意力机制)**：
  - **原理**：利用自注意力机制（Self-Attention）来捕捉序列中任意两个项目之间的关系，而不局限于相邻项目。
  - **优势**：在长序列上表现更好，能够并行计算，训练效率高。非常适合捕捉不同时间点购买的商品之间的复杂关联。
  - **如何应用**：将用户历史购买序列输入到Transformer编码器中，输出每个项目在序列中的表示，然后用这些表示来预测下一个项目。
  - **代表模型**：BERT4Rec、SASRec。这类模型在最近的比赛中通常表现优异。

### 2. 基于图神经网络（GNN）的模型

如果你的数据中用户和商品之间的关系可以被建模为一张图，GNN也是一个非常强大的选择。

- **原理**：将用户、商品看作图中的节点，购买行为看作边。GNN通过在图上进行信息传播和聚合来学习用户和商品的表示。
- **优势**：能够同时利用序列信息和协同过滤信息。例如，不仅学习“用户A”的购买习惯，还能学习和“用户A”行为相似的其他用户的习惯。
- **如何应用**：构建一个包含用户节点和商品节点的二部图。GNN在图上学习节点嵌入（embedding），然后用这些嵌入来预测下一个项目。

### 3. 经典的序列模型

如果你的计算资源有限或想快速建立基线模型，这些方法也是不错的选择。

- **马尔可夫链 (Markov Chain)**：
  - **原理**：基于一个简单的假设：用户的下一个购买行为只依赖于他最近一次的购买行为。
  - **优势**：简单、快速、易于实现。可以作为一个强大的基线模型。
  - **如何应用**：计算商品之间的转移概率，即“如果用户购买了A，那么他接下来购买B的概率是多少”。
  - **代表模型**：FPMC (Factorizing Personalized Markov Chains) 等。

### 4. 基于协同过滤（Collaborative Filtering）的模型

虽然这个任务是基于序列的，但传统的协同过滤方法也可以作为基线或辅助模型。

- **协同过滤**：
  - **原理**：基于“物以类聚，人以群分”的思想。用户相似的人会购买相似的商品，或者相似的商品会被相似的人购买。
  - **如何应用**：通过用户-商品交互矩阵，计算用户或商品之间的相似度。然后，根据相似用户的历史购买记录来推荐。
  - **局限性**：协同过滤通常不考虑购买的时间顺序，只关注“买了什么”，不关注“何时买的”。因此，作为主模型可能效果不佳，但可以与其他方法结合。

### 总结和建议

1. **首选模型：基于 Transformer 的模型（如 SASRec）**。它们在许多序列推荐任务中都取得了最先进的成果，并且能够有效地处理您的数据格式。
2. **如果需要快速启动：使用基于 GRU/RNN 的模型（如 GRU4Rec）或简单的马尔可夫链模型**。它们实现起来相对简单，可以作为强大的基线来评估更复杂模型的改进效果。
3. **数据预处理**：
   - **`buyer_id` 和 `item_id`**：需要将这些 ID 映射为连续的整数，以便作为模型的输入。
   - **`create_order_time`**：这个字段非常关键。你需要用它来对每个用户的购买记录进行**排序**，从而构建正确的购买序列。
   - **`irank`**：这个字段似乎已经帮你完成了排序，所以你可以直接使用它来构建序列。
   - **输入格式**：对于深度学习模型，你需要为每个用户生成一个`[item_1, item_2, ..., item_N]`的序列作为输入。

总而言之，您的任务非常适合用**深度学习中的序列模型**来解决。选择一个能够处理序列数据的模型，并利用您数据中提供的`create_order_time`或`irank`信息来正确构建输入序列，是成功的关键。

### 序列模型

- 指的是专门处理**序列数据（Sequence Data）**的机器学习模型。

#### 什么是序列数据？

序列数据是指一系列相互关联、并且**具有先后顺序**的数据点。这种顺序是数据本身的关键特征，如果打乱顺序，数据的意义就会发生改变。

**常见的序列数据类型包括：**

- **文本**：一个句子中的单词是有顺序的，比如“我爱中国”和“中国爱我”意义完全不同。
- **语音**：一段音频是由一系列声波信号构成的。
- **时间序列**：股票价格、天气数据等，都有严格的时间顺序。
- **用户行为序列**：用户的购买历史、点击历史等，是按照时间先后发生的。

#### 序列模型是如何工作的？

传统的机器学习模型（比如线性回归、决策树）通常假设数据点是独立的。但序列模型则不然，它被设计来**捕捉数据点之间的时序依赖关系**。

举个例子，要预测一个用户接下来会买什么商品，一个好的模型不应该只看他买过的所有商品，它还应该考虑他是**按什么顺序**购买的。

- **用户A**的购买历史是：`[手机] -> [手机壳] -> [耳机]`。
- **用户B**的购买历史是：`[耳机] -> [手机壳] -> [手机]`。

虽然他们买了同样的商品，但购买顺序不同，这暗示了不同的购买意图。一个好的序列模型能够识别出`[手机]`之后，用户很可能需要`[手机壳]`，而`[耳机]`之后可能就没有这种强烈的关联。

序列模型的核心就是拥有一个**“记忆”**功能，能够将之前看到的数据点的信息传递给后面的处理步骤。

#### 序列模型能做什么？

序列模型在很多领域都有广泛应用，主要用于以下任务：

1. **预测 (Prediction)**
   - **下一个项目预测**：正如你的任务，预测用户接下来会买什么、看什么。
   - **股票价格预测**：根据过去的股价走势，预测未来的价格。
   - **天气预报**：根据过去的天气数据，预测未来的天气。
2. **生成 (Generation)**
   - **文本生成**：根据给定的开头，生成连贯的文章或诗歌。
   - **语音合成**：将文本转换为自然的语音。
3. **分类与识别 (Classification & Recognition)**
   - **情感分析**：判断一段文本是积极的还是消极的。
   - **语音识别**：将语音信号转换为文字。
   - **行为识别**：通过穿戴设备记录的运动数据，识别用户的行为（走路、跑步等）。

#### 序列模型的具体例子

在你的预测用户购买行为的任务中，我们提到了一些具体的序列模型，它们都基于不同的技术来实现这种“记忆”功能：

- **RNN (循环神经网络)**：就像一个**链条**，每个单元处理一个数据点，并将上一个单元的“记忆”（隐藏状态）传递给下一个。
- **GRU/LSTM**：是RNN的升级版，解决了长序列中“记忆”衰减的问题，能够更好地记住很久以前的信息。
- **Transformer**：通过“注意力机制”，让模型在处理一个数据点时，能够**同时关注到序列中所有其他数据点**，并根据其重要性赋予不同的权重。这使得它在处理长序列时非常强大。

总而言之，“序列模型”是处理有时间或顺序关系的数据的工具。在你的任务中，它能够学习用户的购买模式，并利用这些模式来预测下一个购买行为，这比忽略顺序的传统方法要有效得多。

# 文件结构

```shell
CustBuyForecast/
│
├── data/                # 数据文件夹（原始数据、处理后数据，已在.gitignore中）
│   ├── raw/             # 原始数据（如train.csv, test_without_last.csv, item_attr.csv）
│   └── processed/       # 处理后的数据（如特征工程结果）
│
├── notebooks/           # Jupyter Notebook文件（如EDA、建模实验等）
│   ├── 01_eda.ipynb
│   ├── 02_feature_engineering.ipynb
│   └── 03_modeling.ipynb
│
├── src/                 # 源代码（Python脚本/模块）
│   ├── __init__.py
│   ├── data_preprocessing.py
│   ├── feature_engineering.py
│   ├── train.py
│   ├── predict.py
│   └── utils.py
│
├── output/              # 输出结果（如模型、提交文件、日志等）
│   ├── models/          # 保存训练好的模型
│   ├── submissions/     # 提交文件
│   └── logs/            # 日志文件
│
├── tests/               # 单元测试
│   └── test_feature_engineering.py
│
├── requirements.txt     # Python依赖包列表
├── README.md            # 项目说明文档
├── .gitignore           # Git忽略文件
└── LICENSE              # 许可证
```

# 安装步骤

- **uv安装**

```powershell
1. uv venv --python 3.9  # 创建.venv
2. uv pip install -r requirements.txt # 安装依赖
3. .venv\Scripts\activate # 激活环境
```